{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = './data'\n",
    "import os\n",
    "import json\n",
    "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import  GPT2LMHeadModel, GPT2Tokenizer, GPT2Config\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data and structure them\n",
    "if os.path.isdir(folder_path):\n",
    "    file_list = os.listdir(folder_path)\n",
    "    data = []\n",
    "    for file_name in file_list:\n",
    "        if file_name.endswith(\".txt\"):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                content = file.read()\n",
    "                title, text = content.split('\\n\\n', 1)\n",
    "                data_row = {}\n",
    "                data_row[\"[TEXT]\"] = text\n",
    "                data_row[\"[TITLE]\"] = title\n",
    "                data.append(data_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing\n",
    "for data_sample in data:\n",
    "    data_sample[\"[TEXT]\"]=data_sample[\"[TEXT]\"].replace('\\n', '')\n",
    "    data_sample[\"[TEXT]\"]=data_sample[\"[TEXT]\"].replace('\\n\\n', '')\n",
    "\n",
    "    tokenized_text = tokenizer.tokenize(data_sample[\"[TEXT]\"])\n",
    "    stopwords_set = set(stopwords.words('english'))\n",
    "    data_sample[\"[TEXT]\"] = [token for token in tokenized_text if token not in stopwords_set]\n",
    "    data_sample[\"[TEXT]\"] = [token for token in tokenized_text if token.isalnum()]\n",
    "    data_sample[\"[TEXT]\"] = ' '.join(data_sample[\"[TEXT]\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save preprocessed\n",
    "data_to_save = json.dumps(data)\n",
    "file_path = 'preprocessed_data_dict.txt'\n",
    "with open(file_path, 'w') as file:\n",
    "    file.write(data_to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict_path = './preprocessed_data_dict.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data\n",
    "def load_dataset(file_path, tokenizer, block_size = 128):\n",
    "    dataset = TextDataset(\n",
    "        tokenizer = tokenizer,\n",
    "        file_path = file_path,\n",
    "        block_size = block_size,\n",
    "    )\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_collator(tokenizer, mlm = False):\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, \n",
    "        mlm=mlm,\n",
    "    )\n",
    "    return data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model, tokenizer definition\n",
    "def train(train_file_path,model_name,\n",
    "          output_dir,\n",
    "          overwrite_output_dir,\n",
    "          per_device_train_batch_size,\n",
    "          num_train_epochs,\n",
    "          ):\n",
    "  tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "  config = GPT2Config.from_pretrained(model_name)\n",
    "  train_dataset = load_dataset(train_file_path, tokenizer)\n",
    "  data_collator = load_data_collator(tokenizer)\n",
    "\n",
    "  tokenizer.save_pretrained(output_dir)\n",
    "      \n",
    "  model = GPT2LMHeadModel.from_pretrained(pretrained_model_name_or_path = model_name, config = config)\n",
    "\n",
    "  model.save_pretrained(output_dir)\n",
    "\n",
    "  training_args = TrainingArguments(\n",
    "          output_dir=output_dir,\n",
    "          overwrite_output_dir=overwrite_output_dir,\n",
    "          per_device_train_batch_size=per_device_train_batch_size,\n",
    "          num_train_epochs=num_train_epochs,\n",
    "      )\n",
    "\n",
    "  trainer = Trainer(\n",
    "          model=model,\n",
    "          args=training_args,\n",
    "          data_collator=data_collator,\n",
    "          train_dataset=train_dataset,\n",
    "  )\n",
    "      \n",
    "  trainer.train()\n",
    "  trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model parameters\n",
    "model_name = 'gpt2'\n",
    "output_dir = './training_out'\n",
    "per_device_train_batch_size = 32\n",
    "num_train_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(\n",
    "    train_file_path=data_dict_path,\n",
    "    model_name=model_name,\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    num_train_epochs=num_train_epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inference\n",
    "def load_model(model_path):\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "    return model\n",
    "\n",
    "def load_tokenizer(tokenizer_path):\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(tokenizer_path)\n",
    "    return tokenizer\n",
    "\n",
    "def generate_text(model_path, sequence):\n",
    "    model = load_model(model_path)\n",
    "    tokenizer = load_tokenizer(model_path)\n",
    "    ids = tokenizer.encode(f'{sequence}', return_tensors='pt')\n",
    "    final_outputs = model.generate(\n",
    "        ids,\n",
    "        do_sample=True,\n",
    "        pad_token_id=model.config.eos_token_id,\n",
    "        top_k=20,\n",
    "        top_p=0.5,\n",
    "        num_return_sequences=6\n",
    "    )\n",
    "    print(tokenizer.decode(final_outputs[0], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inference sample\n",
    "trained_model_path = './training_out'\n",
    "sequence_to_test = '{\"[TEXT]\": German airline Lufthansa has returned to profit in 2004 after posting huge losses in 2003. In a preliminary report, the airline announced net profits of 400m euros ($527.61m; Â£274.73m), compared with a loss of 984m euros in 2003. Operating profits were at 380m euros, ten times more than in 2003. Lufthansa was hit in 2003 by tough competition and a dip in demand following the Iraq war and the killer SARS virus. It was also hit by troubles at its US catering business. Last year, Lufthansa showed signs of recovery even as some European and US airlines were teetering on the brink of bankruptcy. The board of Lufthansa has recommended paying a 2004 dividend of 0.30 euros per share. In 2003, shareholders did not get a dividend. The company said that it will give all the details of its 2004 results on 23 March, \"[TITLE]\":   '\n",
    "generate_text(trained_model_path, sequence_to_test) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "besttask",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
