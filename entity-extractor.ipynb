{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "import openai\n",
    "openai.api_key = 'key'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_path = './data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read .txt files and arrange them in list of dicts\n",
    "if os.path.isdir(input_data_path):\n",
    "    file_list = os.listdir(input_data_path)\n",
    "    data = []\n",
    "    for file_name in file_list:\n",
    "        if file_name.endswith(\".txt\"):\n",
    "            file_path = os.path.join(input_data_path, file_name)\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                content = file.read()\n",
    "                title, text = content.split('\\n\\n', 1)\n",
    "                data_row = {}\n",
    "                data_row[\"text\"] = text\n",
    "                data.append(data_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess text\n",
    "for data_sample in data:\n",
    "    # 1. remove unnecessary formatting\n",
    "    data_sample[\"text\"]=data_sample[\"text\"].replace('\\n', '')\n",
    "    data_sample[\"text\"]=data_sample[\"text\"].replace('\\n\\n', '')\n",
    "    # tokenixe text and remove stopwords and special characters\n",
    "    tokenized_text = tokenizer.tokenize(data_sample[\"text\"])\n",
    "    stopwords_set = set(stopwords.words('english'))\n",
    "    data_sample[\"text\"] = [token for token in tokenized_text if token not in stopwords_set]\n",
    "    data_sample[\"text\"] = [token for token in tokenized_text if token.isalnum()]\n",
    "    data_sample[\"text\"] = ' '.join(data_sample[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define business entities. Adapt to task needs\n",
    "business_entities = [\"agriculture\", 'mining', 'oil and gas extraction', 'construction', 'manufacturing', 'retail', 'information', 'finance', 'real estate', 'health care', 'public administration', 'automotive', 'aerospace', 'transportation', 'telecommunications', 'banking', 'security', 'insurance', 'rental and leasing services', 'legal services', 'accounting', 'tax preparation', 'architecture', 'engineering', 'office administrative services', 'education', 'arts and entertainment', 'food services', 'defence']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_notes_with_entities = []\n",
    "for i in data:\n",
    "    # prompt - define system role\n",
    "    messages = [{\"role\": \"system\", \"content\": f\"\"\"You are a business entity extractor. Analyze the given text and suggest the business industry from provided list that best fits the text.\\\n",
    "           business entities: {business_entities},\n",
    "           You are given a text in a format 'text:' ''' text to analyze '''.\n",
    "           Respond in a pattern: ''' choosen entity ''' \"\"\"}]\n",
    "    message = i['text']\n",
    "    messages.append({\"role\": \"user\", \"content\": message})\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages,\n",
    "    )\n",
    "    GPT_reply = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    messages.append({\"role\": \"assistant\", \"content\": GPT_reply})\n",
    "    data_row = {}\n",
    "    data_row['TEXT'] = i['text']\n",
    "    data_row['ENTITY'] = GPT_reply\n",
    "    business_notes_with_entities.append(data_row)\n",
    "    messages = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save gpt outputs\n",
    "output_data_path = './extracted-entities'\n",
    "with open(output_data_path, 'w') as json_file:\n",
    "    json.dump(business_notes_with_entities, json_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "besttask",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
